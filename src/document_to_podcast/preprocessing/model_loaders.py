from dataclasses import dataclass, field
from types import MappingProxyType
from typing import Protocol

import torch
from kokoro import KPipeline
from llama_cpp import Llama


def load_llama_cpp_model(model_id: str) -> Llama:
    """Loads the given model_id using Llama.from_pretrained.

    Examples:
        >>> model = load_llama_cpp_model("bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf")

    Args:
        model_id (str): The model id to load.
            Format is expected to be `{org}/{repo}/{filename}`.

    Returns:
        Llama: The loaded model.
    """
    org, repo, filename = model_id.split("/")
    model = Llama.from_pretrained(
        repo_id=f"{org}/{repo}",
        filename=filename,
        n_ctx=0,  # 0 means that the model limit will be used, instead of the default (512) or other hardcoded value
        verbose=False,
        n_gpu_layers=-1 if torch.cuda.is_available() else 0,
    )
    return model


@dataclass
class TTSModel:
    """The purpose of this class is to provide a unified interface for all the TTS models supported.
    Specifically, different TTS model families have different peculiarities, for example, the bark models need a
    BarkProcessor, the parler models need their own tokenizer, etc. This wrapper takes care of this complexity so that
    the user doesn't have to deal with it.

    Args:
        model: A TTS model that has a .generate() method or similar
            that takes text as input, and returns an audio in the form of a numpy array.
        model_id (str): The model's identifier string.
        sample_rate (int): The sample rate of the audio, required for properly saving the audio to a file.
        custom_args (dict): Any model-specific arguments that a TTS model might require, e.g. tokenizer.
    """

    model: KPipeline
    model_id: str
    sample_rate: int
    custom_args: field(default_factory=dict)


class TTSLoaderFn(Protocol):
    def __call__(self, model_id: str, **kwargs) -> TTSModel: ...


def _load_kokoro_tts(model_id: str, **kwargs) -> TTSModel:
    """Loads the kokoro model using the KPipeline from the package https://github.com/hexgrad/kokoro

    Args:
        model_id (str): Identifier for a specific model. Kokoro currently only supports one model.
        kwargs (str): Needs to include 'lang_code' necessary to set the language used for generation. For example:
            ðŸ‡ªðŸ‡¸ 'e' => Spanish es
            ðŸ‡«ðŸ‡· 'f' => French fr-fr
            ðŸ‡®ðŸ‡³ 'h' => Hindi hi
            ðŸ‡®ðŸ‡¹ 'i' => Italian it
            ðŸ‡§ðŸ‡· 'p' => Brazilian Portuguese pt-br
            ðŸ‡ºðŸ‡¸ 'a' => American English
            ðŸ‡¬ðŸ‡§ 'b' => British English
            ðŸ‡¯ðŸ‡µ 'j' => Japanese: you will need to also pip install misaki[ja]
            ðŸ‡¨ðŸ‡³ 'z' => Mandarin Chinese: you will need to also pip install misaki[zh]

    Returns:
        TTSModel: The loaded model using the TTSModel wrapper.
    """
    from kokoro import KPipeline

    # If language code not supplied, assume British English
    pipeline = KPipeline(repo_id=model_id, lang_code=kwargs.pop("lang_code", "b"))
    return TTSModel(
        model=pipeline,
        model_id=model_id,
        sample_rate=24000,  # Kokoro's default sample rate
        custom_args={},
    )


_TTS_LOADERS = MappingProxyType(
    {
        # To add support for your model, add it here in the format {model_id} : _load_function
        "hexgrad/Kokoro-82M": _load_kokoro_tts,
    }
)


def tts_loader_by_model(model_id: str) -> TTSLoaderFn:
    loader = _TTS_LOADERS.get(model_id)
    if loader is None:
        raise ValueError(f"Could not load {model_id}.")
    return loader
